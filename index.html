<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jeongyun Kim</title>
  
  <meta name="author" content="Jeongyun Kim">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jeongyun Kim</name>
              </p>
              <p> I am Ph.D. student in Mechanical Engineering at the <a href="https://rpm.snu.ac.kr/index.php/Main/About"> RPM Robotics lab</a>, <a href="https://me.snu.ac.kr/en/"> Seoul National University</a>, advised by Prof. <a href="https://ayoungk.github.io/"> Ayoung Kim</a>.
              </p>
              <p></p>
              <p>
                My research interests lie in perception and depth reconstruction for transparent object manipulation, with a particular focus on vision-based neural volumetric synthesis techniques.

              </p>
              <p style="text-align:center">
                <a href="mailto:jeongyunkim@snu.ac.kr">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=vW2JtFAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/jeongyun0609">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/eongyun-kim-17a2b0349/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/jeongyunkim.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/jeongyunkim.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <tr class="publication">
              <td class="pub-image">
                <img src="images/TRansPose.gif" alt="TRansPose" width="220">
              </td>
              <td class="pub-text">
                TRansPose: Large-scale Multispectral Dataset for Transparent object
                <br>
                <strong>Jeongyun Kim</strong>,
                Myung-Hwan Jeon, Sangwoo Jung, Wooseong Yang, Minwoo Jung, Jaeho Shin and Ayoung Kim
                <br>
                <em> The International Journal of Robotics Research (IJRR)</em>, 2024
                <br>
                [<a href="https://arxiv.org/abs/2307.05016">arXiv</a>]
                [<a href="https://sites.google.com/view/transpose-dataset/home?authuser=0">dataset site</a>]
                <p></p>
                </td>
              </td>
            </tr>

          <tr>
            <tr class="publication">
              <td class="pub-image">
                <img src="images/transplat.png" alt="Transplat" width="220">
              </td>
              <td class="pub-text">
                <strong>TranSplat:</strong> Surface Embedding-guided 3D Gaussian Splatting for Transparent Object Manipulation
                <br>
                <strong>Jeongyun Kim</strong>,
                Jeongho Noh, DongGuw Lee and Ayoung Kim        
                <br>
                <em> IEEE International Conference on Robotics and Automation (ICRA)</em>, 2025
                <br>
                [<a href="https://arxiv.org/abs/2502.07840">arXiv</a>]
                [<a href="https://www.youtube.com/watch?v=O_atdUlaF4I">video</a>]
                [<a href="https://github.com/jeongyun0609/TranSplat?tab=readme-ov-file">code</a>]
                <p></p>
                </td>
              </td>
          </tr>
     
          <tr>
            <tr class="publication">
              <td class="pub-image">
                <img src="images/tranD.png" alt="TranD" width="220">
              </td>
              <td class="pub-text">
                <strong>TranD:</strong> 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update
                <br>
                <strong>Jeongyun Kim</strong>,
                Seunghoon Jeong, Giseop Kim, Myung-Hwan Jeon, Eunji Jun and Ayoung Kim        
                <br>
                <em> IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2025
                <br>
                Code and arXiv are comming soon!
                <!-- [<a href="https://arxiv.org/abs/2502.07840">arXiv</a>]
                [<a href="https://www.youtube.com/watch?v=O_atdUlaF4I">video</a>]
                [<a href="https://github.com/jeongyun0609/TranSplat?tab=readme-ov-file">code</a>] -->
                <p></p>
                </td>
              </td>
          </tr>          


          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Other publications and projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SThereo.png" alt="STheReO" width="220">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              STheReO: Stereo Thermal Dataset for Research in Odometry and Mapping
            <br>
            Seungsang Yun, Minwoo Jung,
            <strong>Jeongyun Kim</strong>,
            Sangwoo Jung, Younghun Cho, Myung-Hwan Jeon, Giseop Kim and Ayoung Kim
            <br>
            <em> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2022
            <br>
            [<a href="https://ieeexplore.ieee.org/abstract/document/9981857">paper</a>]
            [<a href="https://sites.google.com/view/rpmsthereo/">dataset site</a>]
            <p></p>
            <p>
              <strong>STheReO</strong> introduces a stereo thermal camera dataset (STheReO) with multiple navigation sensors to encourage thermal SLAM researches. 
            </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/prima6d.png" alt="PrimA6D++" width="220">
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              Ambiguity-aware multi-object pose optimization for visually-assisted robot manipulation
            </a>
            <br>
            Myung-Hwan Jeon
            <strong>Jeongyun Kim</strong>,
            Jee-Hwan Ryu, and Ayoung Kim        
            <br>
            <em> IEEE Robotics and Automation Letters (RA-L)</em>, 2023
            <br>
            [<a href="https://ieeexplore.ieee.org/abstract/document/9954132">paper</a>]
            [<a href="https://www.youtube.com/watch?v=akbI61jUJgY">video</a>]
            [<a href="https://github.com/MyungHwanJeon/PrimA6D">code</a>]
            <p></p>
            <p>
              PrimA6D++ is an ambiguity-aware 6D pose estimation network that predicts uncertainty along three rotation‐axis primitives and integrates these estimates into a SLAM‐style multi‐object pose optimization, achieving significant accuracy gains while enabling real‐time robot manipulation.
            </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Thermal_chameleon.png" alt="ThermalChameleon" width="220">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              Thermal Chameleon: Task-Adaptive Tone-Mapping for Radiometric Thermal-Infrared Images
            </a>
            <br>
            Dong-Guw Lee,
            <strong>Jeongyun Kim</strong>,
            Younggun Cho and Ayoung Kim        
            <br>
            <em> IEEE Robotics and Automation Letters (RA-L)</em>, 2024
            <br>
            [<a href="https://ieeexplore.ieee.org/abstract/document/10715649">paper</a>]
            [<a href="https://github.com/donkeymouse/ThermalChameleon">code</a>]
            <p></p>
            <p>
              TCNet is a task-adaptive tone-mapping network for RAW 14-bit thermal infrared images that generates tailored representations for each downstream task—such as object detection or monocular depth estimation—thereby removing heuristic preprocessing and temperature priors while improving generalization with minimal overhead.
            </p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Template by Jon Barron.</a>
             </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table> -->
</body>

</html>
